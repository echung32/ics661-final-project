# Comparative Analysis of Prompting Techniques for Text Classification Across Multiple LLMs

<br/>

<details>
  <summary>Table of Contents</summary>
  <ul>
    <li>
      <a href="#goal">Goal</a>
    </li>
    <li>
      <a href="#methodology">Methodology</a>
    </li>
    <li>
      <a href="#team-members">Team Members</a>
    </li>
  </ul>
</details>

<!--
## Table of Contents
- [Goal](#goal)
- [Methodology](#methodology)
- [Team Members](#team-members)
-->

## Goal
The goal of this project is to compare prompting techniques on various datasets with different models, and to gain insight into the difference in performance on different models and datasets.

Due to the complexity of natural language, with correct categorization relying on understanding the nuanced context of a word or sentence, text classification is a challenging task for language models with no inherent understanding of language. This challenge is apparent, and even amplified, when attempting to effectively prompt these models for some specific task.

## Methodology

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f

f
## Team Members

- Ethan Chung
- Anika Rahman Joyita
- Jayden Carl Tactay
- Samuel Yang
