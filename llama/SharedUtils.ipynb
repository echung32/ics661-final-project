{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf4200-e5bd-4838-8e52-9b66219b6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "from time import time\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a39a2-2596-4718-835e-8d6619cf9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(data):\n",
    "  data[\"label_human\"] = label_map[data[\"label\"]]\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff8c4a-ad25-4801-8e54-e502484d85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_examples(dataset, key, n_per_label=1):\n",
    "    random.seed(42)\n",
    "\n",
    "    examples_by_label = {}\n",
    "    for example in dataset:\n",
    "        label = example[\"label\"]\n",
    "        if label not in examples_by_label:\n",
    "            examples_by_label[label] = []\n",
    "        examples_by_label[label].append(example)\n",
    "\n",
    "    few_shot_examples = []\n",
    "    for label, examples in examples_by_label.items():\n",
    "        few_shot_examples.extend(random.sample(examples, n_per_label))\n",
    "\n",
    "    examples_text = \"\\n\".join(\n",
    "        f\"Sentence: {example[key]}\\n\"\n",
    "        f\"Sentiment: {example['label_human']}\"\n",
    "        for example in few_shot_examples\n",
    "    )\n",
    "\n",
    "    examples_text_cot = \"\\n\".join(\n",
    "        f\"Sentence: {example[key]}\\n\"\n",
    "        f\"Reasoning: {get_reasoning(example['label'])}\\n\"\n",
    "        f\"Sentiment: {example['label_human']}\"\n",
    "        for example in few_shot_examples\n",
    "    )\n",
    "\n",
    "    return few_shot_examples, examples_text, examples_text_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e84256-c851-4c1e-b90e-fc0ee21bb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_test, y_test, fn, examples):\n",
    "    t0 = time()\n",
    "    predictions = []\n",
    "\n",
    "    for idx, (sentence, true_label) in enumerate(zip(X_test, y_test), 1):\n",
    "        prediction = fn(sentence, examples)\n",
    "        sentiment = parse_sentiment(prediction)\n",
    "\n",
    "        predictions.append(sentiment)\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"---\\nSentence: {sentence}\\nTrue: {true_label}\\nPrediction: {sentiment}\")\n",
    "            print(f\"Processed {idx}/{len(dataset)} examples, Time: {time()-t0:.3f}\\n---\\n\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504d438-d16a-4add-9b92-65fdc5bfde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_sentiment_reason(response):\n",
    "#     sentiment_pattern = r\"Sentiment:\\s*(.*?)\\n\"\n",
    "#     reason_pattern = r\"Reason:\\s*(.*?)$\"\n",
    "    \n",
    "#     sentiment_matches = re.findall(sentiment_pattern, response)\n",
    "#     reason_match = re.search(reason_pattern, response)\n",
    "\n",
    "#     sentiment = sentiment_matches[1].strip().lower() if len(sentiment_matches) > 1 else None\n",
    "#     reason = reason_match.group(1).strip() if reason_match else None\n",
    "    \n",
    "#     return (sentiment, reason)\n",
    "\n",
    "def parse_sentiment(response):\n",
    "    sentiment_pattern = r\"Sentiment:\\s*(.*?)$\"\n",
    "    sentiment_match = re.search(sentiment_pattern, response)\n",
    "\n",
    "    sentiment = sentiment_match.group(1).strip().lower() if sentiment_match else \"invalid\"\n",
    "\n",
    "    # Sometimes, model outputs in an incorrect format (e.g. \"determination/resilience (however, closest match from your options would be: joy)\")\n",
    "    # This will make it simply identify as None rather than trying to deal with parsing the output.\n",
    "    if sentiment not in [label for label in label_map.values()]:\n",
    "        # print(f\"+++\\nResponse in invalid format:\\n\\n{response}\\n\\n---\\n\")\n",
    "        sentiment = \"invalid\"\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037cac8-8aaf-431e-a63a-3f95653da5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot(content: str, examples = None):\n",
    "    # Asking for reasoning increases prompt response time by 10x.\n",
    "    # f\"Now classify the following sentence and provide reasoning for your classification. The output MUST follow this format:\\n\"\n",
    "    # f\"Sentiment: [Classification]\\nReason: [Explanation]\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=f\"Your goal is to read a sentence and classify its sentiment into one of the following categories: {', '.join(label_map.values())}.\\n\\n\"\n",
    "                    f\"Now classify the following sentence. The output MUST follow this format:\\n\"\n",
    "                    f\"Sentiment: [Classification]\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=content\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    response = chat_model.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "def few_shot(content: str, examples: str):\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=f\"Your goal is to read a sentence and classify its sentiment into one of the following categories: {', '.join(label_map.values())}.\\n\\n\"\n",
    "                    f\"Here are some examples:\\n{examples}\\n\\n\"\n",
    "                    f\"Now classify the following sentence. The output MUST follow this format:\\n\"\n",
    "                    f\"Sentiment: [Classification]\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=content\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    response = chat_model.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd93320-075e-41a3-8f2f-9e23bf11e9f6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "def create_reports(y_test, predictions):\n",
    "    y_test, y_pred = filter_invalid(y_test, predictions)\n",
    "\n",
    "    calc_accuracy(y_test, y_pred)\n",
    "    class_report(y_test, y_pred)\n",
    "    conf_matrix(y_test, y_pred)\n",
    "    acc_graph(y_test, y_pred)\n",
    "\n",
    "def filter_invalid(y_test, y_pred):\n",
    "    filtered_y_test = []\n",
    "    filtered_y_pred = []\n",
    "    for true_label, pred_label in zip(y_test, y_pred):\n",
    "        if pred_label != \"invalid\":\n",
    "            filtered_y_test.append(true_label)\n",
    "            filtered_y_pred.append(pred_label)\n",
    "    print(f\"Filtered {len(y_test) - len(filtered_y_test)} invalid predictions.\")\n",
    "\n",
    "    return filtered_y_test, filtered_y_pred\n",
    "\n",
    "def calc_accuracy(y_test, y_pred):\n",
    "    accuracy = sum([1 if p == t else 0 for p, t in zip(y_pred, y_test)]) / len(y_test) * 100\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "def class_report(y_test, y_pred):\n",
    "    print(classification_report(\n",
    "            y_test, y_pred,\n",
    "            labels=label_values))\n",
    "\n",
    "def conf_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=label_values)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "def acc_graph(y_test, y_pred):\n",
    "    accuracies = np.cumsum(np.array(y_test) == np.array(y_pred)) / np.arange(1, len(y_test) + 1)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(accuracies * 100)\n",
    "    plt.title(\"Accuracy Over Samples\")\n",
    "    plt.xlabel(\"Number of Samples\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
